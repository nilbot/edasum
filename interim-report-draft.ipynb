{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic description\n",
    "\n",
    "# Text summarization \n",
    "\n",
    "\n",
    "## Summarization of review (time-series of opinions)\n",
    "\n",
    "# Approaches\n",
    "There are two types of summarization: extractive and abstractive\n",
    "## Sentence selection\n",
    "Produce a summarization based on representative sentences which cover important aspects. \n",
    "\n",
    "- intuitive approach\n",
    "- heavy manual engineering\n",
    "\n",
    "## Temporal weighted selection/extraction\n",
    "\n",
    "- focused on sentiment\n",
    "- linear combination of tf-idf and other ranking (performant)\n",
    "- topic tracking is not covered\n",
    "- retrieval only\n",
    "\n",
    "## learn from textual structure, probabilistic graphs\n",
    "\n",
    "- complex\n",
    "- summarize base on pure NLP (sentence and keyword relation as graph)\n",
    "- voting ensemble for final ranking\n",
    "\n",
    "other more well received approach in this statistical domain is\n",
    "http://www.sciencedirect.com/science/article/pii/S1568494615001039\n",
    "\n",
    "## learn from text, deep (reinforcement) learning\n",
    "state of art deep learning (using RNN) is covered by\n",
    "http://www.aclweb.org/anthology/D15-1044\n",
    "\n",
    "- supervised learning, means it requires large labeled dataset\n",
    "\n",
    "possible unsupervised learning would be using reinforcement learning deep neural networks, however, no notable work yet has been published on this topic.\n",
    "\n",
    "the follow-up of that RNN approach has been dissected in http://arxiv.org/pdf/1602.06023v2.pdf\n",
    "such that:\n",
    "1. Like NAMAS - they use an Attention model in the encoder-decoder\n",
    "2. They use the Large vocabulary trick (LVT) of Jean et al 2014 http://arxiv.org/abs/1412.2007 - which means when you decode, use only the words that appear in the source - this reduces perplexity\n",
    "3. But then you lose the capability to do \"abstractive\" summary - which means introducing words which were not in the source - so they do \"vocabulary expansion\" - by adding a layer of \"word2vec nearest neighbors\" to the words in the input\n",
    "4. Feature rich encoding - they add TF*IDF and Named Entity types to the word embeddings (concatenated) to the encodings of the words - this adds to the encoding dimensions that reflect \"importance\" of the words\n",
    "5. The most interesting of all is what they call the \"Switching Generator/Pointer\" layer (section 3.5).  In the decoder - they add a layer that decides to either generate a new word based on the context / previously generated word (usual decoder) OR copy a word from the input (that is - add a pointer to the input).  They learn when to do Generate vs. Pointer - and when it is a Pointer - to which word of the input to Point to.  This idea is from [Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba.]\n",
    "\n",
    "\n",
    "# Our Approach\n",
    "\n",
    "align with direction similar to sentence approach, find keypoints first, summarize using alternative NLP technique afterwards.\n",
    "\n",
    "\n",
    "Challenge: \n",
    "\n",
    "* which aspects are important\n",
    "* which reviews are representative\n",
    "\n",
    "\n",
    "### focus on which review\n",
    "\n",
    "* focus on opinion change\n",
    "\n",
    "### focus on what to extract\n",
    "\n",
    "* topic\n",
    "* sentiment\n",
    "\n",
    "## Our progress in exploratory analysis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
