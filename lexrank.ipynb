{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "\n",
    "from collections import *\n",
    "from dataframe import *\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n",
      "test test '\\,\n",
      "[('', '', ''), ('', '', ''), ('', '', ''), ('', '', ''), ('', '', '')]\n"
     ]
    }
   ],
   "source": [
    "# base\n",
    "class TSBase(object):\n",
    "    def build_sentence_db(self, document_set):\n",
    "        \"\"\"\n",
    "        document_set is set or list of (review_id, review_text) tuple.\n",
    "        \"\"\"\n",
    "        # internal document mapping, implicitly index is the internal id, same for ism (sentences)\n",
    "        idm = [document for document in document_set]\n",
    "        ism = [(doc_id, sentence) for doc_id, doc in enumerate(idm) for sentence in nltk.sent_tokenize(doc[1]) ]\n",
    "\n",
    "        self.world_txt = \" \".join([text[1] for text in idm])\n",
    "        self.world_txt = \" \".join(self.world_txt.split())\n",
    "\n",
    "    \n",
    "    def world(self):\n",
    "        return self.world_txt\n",
    "    def words(self):\n",
    "        pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "            ([A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
    "           | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "           | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "           | \\.\\.\\.            # ellipsis\n",
    "           | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    "        '''\n",
    "        self.world_words = nltk.regexp_tokenize(self.world(), pattern)\n",
    "\n",
    "# test for base\n",
    "if not os.path.exists('dataset'):\n",
    "    print('fak, fix it')\n",
    "else:\n",
    "    import codecs\n",
    "    \n",
    "    DOC_PREFIX = 'dataset/text/documents'\n",
    "    txts = os.listdir(DOC_PREFIX)[:6]\n",
    "    counter = 0\n",
    "    docs = deque()\n",
    "    for t in txts:\n",
    "        with codecs.open(os.path.join(DOC_PREFIX,t),'r','utf-8') as f:\n",
    "            raw = f.read()\n",
    "            doc_id = os.path.splitext(os.path.basename(f.name))[0]\n",
    "            print(doc_id)\n",
    "            docs.append((doc_id,raw))\n",
    "            counter+=1\n",
    "    \n",
    "    tsbase = TSBase()\n",
    "    tsbase.build_sentence_db(docs)\n",
    "    print(tsbase.world())\n",
    "    print(tsbase.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# centrality of sentence := \\sum qualified(words); where qualified(w) = if w in pseudo_document ? 1 : 0"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
