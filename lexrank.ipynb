{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "\n",
    "from collections import *\n",
    "from dataframe import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# language lookup table\n",
    "knowledge = {}\n",
    "knowledge[\"haven't\"] = \"have not\"\n",
    "knowledge[\"hasn't\"] = \"has not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"doesn't\"] = \"does not\"\n",
    "knowledge[\"don't\"] = \"do not\"\n",
    "knowledge[\"didn't\"] = \"did not\"\n",
    "knowledge[\"couldn't\"] = \"could not\"\n",
    "knowledge[\"mustn't\"] = \"must not\"\n",
    "knowledge[\"can't\"] = \"can not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"won't\"] = \"will not\"\n",
    "knowledge[\"wouldn't\"] = \"would not\"\n",
    "knowledge[\"i'm\"] = \"i am\"\n",
    "knowledge[\"it's\"] = \"it is\"\n",
    "knowledge[\"let's\"] = \"let us\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# base\n",
    "class TSBase(object):\n",
    "    def build_internal(self, document_set):\n",
    "        \"\"\"\n",
    "        document_set is set or list of (review_id, review_text) tuple.\n",
    "        \"\"\"\n",
    "        # internal document mapping, implicitly index is the internal id, same for ism (sentences)\n",
    "        self._idm = [document for document in document_set]\n",
    "        self._ism = [(doc_id, sentence) for doc_id, doc in enumerate(self._idm) for sentence in nltk.sent_tokenize(doc[1]) ]\n",
    "\n",
    "        self._world = \" \".join([text[1] for text in self._idm])\n",
    "        self._world = \" \".join(self._world.split())\n",
    "        \n",
    "        self._world = self._world.lower()\n",
    "        \n",
    "        # global static look up table for contraction must be present\n",
    "        replace_contraction = re.compile(r'\\b(' + '|'.join(knowledge.keys()) + r')\\b')\n",
    "        self._world = replace_contraction.sub(lambda x: knowledge[x.group()], self._world)\n",
    "        \n",
    "        # caveat orginal inclues\n",
    "        \"\"\"\n",
    "        | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        \"\"\"\n",
    "        pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | \\w+(?:'\\w+)*        # words that have ' in between\n",
    "        '''\n",
    "        self._world_words = nltk.regexp_tokenize(self.world(), pattern)\n",
    "        self._world_filtered_words = [word for word in self._world_words if word not in stopwords.words('english')]\n",
    "        \n",
    "\n",
    "        # world term frequency\n",
    "        self.build_world_tf()\n",
    "        self.build_idf()\n",
    "\n",
    "        # persistence\n",
    "        import pickle\n",
    "\n",
    "        file = open(r'dataset/world_words.pkl', 'wb')\n",
    "        pickle.dump(self._world_words, file)\n",
    "        file.close()\n",
    "        \n",
    "        file = open(r'dataset/world_words_document_matrix.pkl', 'wb')\n",
    "        pickle.dump(self._world_words_document_matrix, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_internal():\n",
    "        #reload object from file\n",
    "        words = open(r'dataset/world_words.pkl', 'rb')\n",
    "        self._world_words = pickle.load(words)\n",
    "        words.close()\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def world(self):\n",
    "        return self._world\n",
    "    def world_words(self):\n",
    "        return self._world_words\n",
    "    def world_filtered_words(self):\n",
    "        return self._world_filtered_words\n",
    "\n",
    "    def world_words_set(self):\n",
    "        if hasattr(self, '_world_words_set') and len(self._world_words_set()) != 0:\n",
    "            return self._world_words_set\n",
    "        else:\n",
    "            self._world_words_set = sorted(list(set(self.world_words())))\n",
    "            return self._world_words_set\n",
    "    def world_filtered_words_set(self):\n",
    "        if hasattr(self, '_world_filtered_words_set')  and len(self._world_filtered_words_set()) != 0:\n",
    "            return self._world_filtered_words_set\n",
    "        else:\n",
    "            self._world_filtered_words_set = sorted(list(set(self.world_filtered_words())))\n",
    "            return self._world_filtered_words_set\n",
    "    \n",
    "    def world_tf(self):\n",
    "        return self._world_tf\n",
    "    def world_filtered_tf(self):\n",
    "        return self._world_filtered_tf\n",
    "    \n",
    "    def build_world_tf(self):\n",
    "        \"\"\"\n",
    "        this is worlds word hash\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        self._world_tf = Counter(self.world_words())\n",
    "        self._world_filtered_tf = Counter(self.world_filtered_words())\n",
    "    \n",
    "    def build_idf(self):\n",
    "        self._world_words_document_matrix = np.zeros((len(self._idm),len(self.world_words_set())))\n",
    "        return self._world_words_document_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "# test for base\n",
    "if not os.path.exists('dataset'):\n",
    "    print('fak, fix it')\n",
    "else:\n",
    "    DOC_PREFIX = 'dataset/text/documents/raw'\n",
    "    txts = os.listdir(DOC_PREFIX)[:6]\n",
    "#     txts = os.listdir(DOC_PREFIX) # all, caution, should use parallelism to speed up\n",
    "    counter = 0\n",
    "    docs = deque()\n",
    "    for t in txts:\n",
    "        with open(os.path.join(DOC_PREFIX,t), 'r') as f:\n",
    "            raw = f.read()\n",
    "            doc_id = os.path.splitext(os.path.basename(f.name))[0]\n",
    "#             print(doc_id)\n",
    "            docs.append((doc_id,raw))\n",
    "            counter+=1\n",
    "    \n",
    "    tsbase = TSBase()\n",
    "    tsbase.build_internal(docs)\n",
    "#     print(tsbase.world())\n",
    "#     print(tsbase.words())\n",
    "#     print(tsbase.world_filtered_tf())\n",
    "    wtf = tsbase.world_tf()\n",
    "    print(len(wtf.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# centrality of sentence := \\sum qualified(words); where qualified(w) = if w in pseudo_document ? 1 : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "nltk.regexp_tokenize(text, pattern)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
