{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "\n",
    "from collections import *\n",
    "from dataframe import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import math\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# language lookup table\n",
    "knowledge = {}\n",
    "knowledge[\"haven't\"] = \"have not\"\n",
    "knowledge[\"hasn't\"] = \"has not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"doesn't\"] = \"does not\"\n",
    "knowledge[\"don't\"] = \"do not\"\n",
    "knowledge[\"didn't\"] = \"did not\"\n",
    "knowledge[\"couldn't\"] = \"could not\"\n",
    "knowledge[\"mustn't\"] = \"must not\"\n",
    "knowledge[\"can't\"] = \"can not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"won't\"] = \"will not\"\n",
    "knowledge[\"wouldn't\"] = \"would not\"\n",
    "knowledge[\"i'm\"] = \"i am\"\n",
    "knowledge[\"it's\"] = \"it is\"\n",
    "knowledge[\"let's\"] = \"let us\"\n",
    "\n",
    "# custom regex tokenizer pattern\n",
    "# caveat: orginal inclues\n",
    "\"\"\"\n",
    "| [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "\"\"\"\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.              # ellipsis\n",
    "  | \\w+(?:'\\w+)*        # words that have ' in between\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TSBase(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def build_internal(self, document_set, remove_stopwords=False):\n",
    "        \"\"\"\n",
    "        document_set is set or list of (review_id, review_text) tuple.\n",
    "        \"\"\"\n",
    "        # internal document mapping, implicitly index is the internal id, same for ism (sentences)\n",
    "        self._idm = [document for document in document_set]\n",
    "        self.save_attr(self._idm, \"idm\")\n",
    "        self._ism = [(doc_id, sentence) for doc_id, doc in enumerate(self._idm) for sentence in nltk.sent_tokenize(doc[1]) ]\n",
    "        self.save_attr(self._ism, \"ism\")\n",
    "        # build world (entire genre), and tf for genre\n",
    "        self.build_world()\n",
    "        self.save_attr(self._world, \"world\")\n",
    "        \n",
    "        self.build_world_tf(remove_stopwords)\n",
    "        self.save_attr(self._world_words, \"world_words\")\n",
    "        self.save_attr(self._world_tf, \"world_tf\")\n",
    "\n",
    "        # build world document matrix based on built words\n",
    "        self.build_idf(remove_stopwords)\n",
    "        self.save_attr(self._idf, \"idf\")\n",
    "\n",
    "    def save_attr(self, attr, attr_str):\n",
    "        file = open(\"dataset/{0}.pkl\".format(attr_str), 'wb')\n",
    "        pickle.dump(attr, file)\n",
    "        file.close()\n",
    "        \n",
    "    def load_attr(self, attr_str):\n",
    "        obj = open(\"dataset/{0}.pkl\".format(attr_str), 'rb')\n",
    "        attr = pickle.load(obj)\n",
    "        obj.close()\n",
    "        return attr\n",
    "        \n",
    "    def load_internal():\n",
    "        self._idm = self.load_attr(\"idm\")\n",
    "        \n",
    "        self._ism = self.load_attr(\"ism\")\n",
    "        \n",
    "        self._world = self.load_attr(\"world\")\n",
    "        \n",
    "        self._world_words = self.load_attr(\"world_words\")\n",
    "        \n",
    "        self._world_tf = self.load_attr(\"world_tf\")\n",
    "        \n",
    "        self._world_words_document_matrix = self.load_attr(\"world_words_document_matrix\")\n",
    "        \n",
    "        self._idf = self.load_attr(\"idf\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    getter methods\n",
    "    \"\"\"\n",
    "    def world(self):\n",
    "        return self._world\n",
    "    \n",
    "    \n",
    "    def world_words(self):\n",
    "        return self._world_words\n",
    "    \n",
    "    \n",
    "    def world_words_set(self):\n",
    "        if hasattr(self, '_world_words_set') and len(self._world_words_set) != 0:\n",
    "            return self._world_words_set\n",
    "        else:\n",
    "            self._world_words_set = sorted(list(set(self.world_words())))\n",
    "            return self._world_words_set\n",
    "        \n",
    "        \n",
    "    def world_tf(self):\n",
    "        return self._world_tf\n",
    "    \n",
    "    def idf(self):\n",
    "        if hasattr(self, '_idf') and len(self._idf) != 0:\n",
    "            return self._idf\n",
    "        return None\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    builder methods\n",
    "    \"\"\"\n",
    "    def build_world(self):\n",
    "        self._world = \" \".join([text[1] for text in self._idm])\n",
    "        self._world = \" \".join(self._world.split())\n",
    "        \n",
    "        preprocessed = self.preprocess(self._world)\n",
    "        \n",
    "        self._world = preprocessed\n",
    "    \n",
    "    def build_world_words(self, remove_stopwords):\n",
    "        self._world_words = self.tokenize(self.world())\n",
    "\n",
    "        if remove_stopwords:\n",
    "            self._world_words = self.nonstop(self._world_words)\n",
    "    \n",
    "    \n",
    "    def build_world_tf(self, remove_stopwords):\n",
    "        self.build_world_words(remove_stopwords)\n",
    "        \"\"\"\n",
    "        this is worlds word hash\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        self._world_tf = Counter(self.world_words())\n",
    "\n",
    "    \n",
    "    def build_idf(self, remove_stopwords):\n",
    "        N = len(self.world_words_set())\n",
    "        m = len(self._idm)\n",
    "        \n",
    "        word_map2index = defaultdict()\n",
    "        \n",
    "        for i in range(N):\n",
    "            word = self.world_words_set()[i]\n",
    "            word_map2index[word] = i\n",
    "        \n",
    "        self._world_words_document_matrix = sparse.dok_matrix((m,N),dtype=np.int)\n",
    "        \n",
    "        for doc_index,doc_tuple in enumerate(self._idm):\n",
    "            tokens = self.tokenize(self.preprocess(doc_tuple[1]))\n",
    "            if remove_stopwords:\n",
    "                tokens = self.nonstop(tokens)\n",
    "            local_tf = Counter(tokens)\n",
    "\n",
    "            for token in set(tokens):\n",
    "                word_index = word_map2index[token]\n",
    "                freq = local_tf[token]\n",
    "                self._world_words_document_matrix[doc_index, word_index] = freq\n",
    "                \n",
    "        self.save_attr(self._world_words_document_matrix, \"world_words_document_matrix\")\n",
    "\n",
    "        self._idf = defaultdict()\n",
    "        for i in range(N):\n",
    "            word = self.world_words_set()[i]\n",
    "            n_i = self._world_words_document_matrix.getcol(i).count_nonzero()\n",
    "            self._idf[word] = math.log( m / float(n_i) , 10)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    utility methods\n",
    "    \"\"\"\n",
    "    def tokenize(self,text):\n",
    "        return nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        \n",
    "        # global static look up table for contraction must be present\n",
    "        replace_contraction = re.compile(r'\\b(' + '|'.join(knowledge.keys()) + r')\\b')\n",
    "        return replace_contraction.sub(lambda x: knowledge[x.group()], text)\n",
    "\n",
    "    \n",
    "    def nonstop(self, tokens):\n",
    "        cachedStopWords = stopwords.words(\"english\")\n",
    "        return [token for token in tokens if token not in cachedStopWords]\n",
    "    \n",
    "        \n",
    "    def hashing_vectorizer(self, text, N):\n",
    "        \"\"\"term frequency (local)\"\"\"\n",
    "        x = np.zeros(N, dtype=np.int32)\n",
    "        words = self.tokenize(text)\n",
    "        for w in words:\n",
    "            h = hash(w)\n",
    "            x[h % N] += 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "small_test = False\n",
    "value_test = False\n",
    "# test for base\n",
    "\n",
    "DOC_PREFIX = 'dataset/text/documents/raw'\n",
    "if small_test:\n",
    "    txts = os.listdir(DOC_PREFIX)[100000:100100]\n",
    "else:\n",
    "    txts = os.listdir(DOC_PREFIX) # all, caution, should use parallelism to speed up\n",
    "counter = 0\n",
    "docs = deque()\n",
    "for t in txts:\n",
    "    with open(os.path.join(DOC_PREFIX,t), 'r') as f:\n",
    "        raw = f.read()\n",
    "        doc_id = os.path.splitext(os.path.basename(f.name))[0]\n",
    "        if small_test and value_test:\n",
    "            print(raw,\"\\n\\n\")\n",
    "        docs.append((doc_id,raw))\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if small_test:\n",
    "    import cProfile\n",
    "    \n",
    "    cProfile.run( 'tsbase = TSBase(); tsbase.build_internal(docs, True)' )\n",
    "else:\n",
    "    tsbase = TSBase()\n",
    "    tsbase.build_internal(docs, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if small_test and value_test:\n",
    "    wtf = tsbase.world_tf()\n",
    "    print(len(wtf.keys()))\n",
    "\n",
    "    idf = tsbase.idf()\n",
    "    print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# centrality of sentence := \\sum qualified(words); where qualified(w) = if w in pseudo_document ? 1 : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "nltk.regexp_tokenize(text, pattern)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
