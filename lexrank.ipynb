{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "\n",
    "from collections import *\n",
    "from dataframe import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# language lookup table\n",
    "knowledge = {}\n",
    "knowledge[\"haven't\"] = \"have not\"\n",
    "knowledge[\"hasn't\"] = \"has not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"doesn't\"] = \"does not\"\n",
    "knowledge[\"don't\"] = \"do not\"\n",
    "knowledge[\"didn't\"] = \"did not\"\n",
    "knowledge[\"couldn't\"] = \"could not\"\n",
    "knowledge[\"mustn't\"] = \"must not\"\n",
    "knowledge[\"can't\"] = \"can not\"\n",
    "knowledge[\"hadn't\"] = \"had not\"\n",
    "knowledge[\"won't\"] = \"will not\"\n",
    "knowledge[\"wouldn't\"] = \"would not\"\n",
    "knowledge[\"i'm\"] = \"i am\"\n",
    "knowledge[\"it's\"] = \"it is\"\n",
    "knowledge[\"let's\"] = \"let us\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'stayed': 4, 'hotel': 4, 'bar': 4, 'location': 4, 'river': 4, 'one': 3, 'food': 3, 'breakfast': 3, 'staff': 3, 'could': 3, 'nice': 3, 'great': 3, 'clean': 3, 'amazing': 2, 'good': 2, 'last': 2, 'christmas': 2, 'experience': 2, 'time': 2, 'place': 2, 'want': 2, 'five': 2, 'minute': 2, 'walk': 2, 'lee': 2, 'excellent': 2, 'city': 2, 'rooms': 2, 'price': 2, 'room': 2, 'perfect': 2, 'dublin': 2, 'night': 1, 'sisters': 1, 'dinner': 1, 'included': 1, 'massage': 1, 'spa': 1, 'heaven': 1, 'def': 1, 'recommend': 1, 'stay': 1, 'back': 1, 'enough': 1, 'help': 1, 'us': 1, 'spotless': 1, 'blessed': 1, 'weather': 1, 'sit': 1, 'outside': 1, 'lovely': 1, 'gardens': 1, 'girlfriend': 1, 'got': 1, 'engaged': 1, 'shelbourne': 1, '2012': 1, 'visited': 1, 'suites': 1, 'life': 1, 'everything': 1, 'class': 1, 'relaxed': 1, 'pint': 1, 'glass': 1, 'wine': 1, 'head': 1, 'horse': 1, 'shoe': 1, 'whilst': 1, 'front': 1, 'little': 1, 'market': 1, 'sophisticated': 1, 'watch': 1, 'celebs': 1, 'also': 1, 'beautiful': 1, 'residents': 1, 'lounge': 1, 'better': 1, 'st': 1, 'stephens': 1, 'green': 1, 'grafton': 1, 'street': 1, 'donoghues': 1, 'theatre': 1, 'within': 1, 'occasions': 1, 'four': 1, 'days': 1, 'business': 1, 'must': 1, 'say': 1, 'never': 1, 'disappoints': 1, 'firstly': 1, 'situated': 1, 'banks': 1, 'cork': 1, 'loveliest': 1, 'parts': 1, 'terrace': 1, 'overlooking': 1, 'leisure': 1, 'centre': 1, 'best': 1, 'ever': 1, 'found': 1, 'including': 1, 'large': 1, 'swimming': 1, 'pool': 1, 'professional': 1, 'eaten': 1, 'restaurant': 1, 'well': 1, 'appointed': 1, 'overall': 1, 'relaxing': 1, 'enjoyable': 1, 'starting': 1, 'positives': 1, 'quick': 1, 'plentiful': 1, 'couple': 1, 'two': 1, 'nights': 1, 'reception': 1, 'complain': 1, 'get': 1, 'order': 1, 'drinks': 1, 'comment': 1, 'quite': 1, 'central': 1, 'especially': 1, 'expensive': 1, 'property': 1, 'located': 1, 'main': 1, 'bus': 1, 'station': 1, '2': 1, 'center': 1, 'town': 1})\n"
     ]
    }
   ],
   "source": [
    "# base\n",
    "class TSBase(object):\n",
    "    def build_sentence_db(self, document_set):\n",
    "        \"\"\"\n",
    "        document_set is set or list of (review_id, review_text) tuple.\n",
    "        \"\"\"\n",
    "        # internal document mapping, implicitly index is the internal id, same for ism (sentences)\n",
    "        idm = [document for document in document_set]\n",
    "        ism = [(doc_id, sentence) for doc_id, doc in enumerate(idm) for sentence in nltk.sent_tokenize(doc[1]) ]\n",
    "\n",
    "        self._world = \" \".join([text[1] for text in idm])\n",
    "        self._world = \" \".join(self._world.split())\n",
    "        \n",
    "        self._world = self._world.lower()\n",
    "        \n",
    "        # global static look up table for contraction must be present\n",
    "        replace_contraction = re.compile(r'\\b(' + '|'.join(knowledge.keys()) + r')\\b')\n",
    "        self._world = replace_contraction.sub(lambda x: knowledge[x.group()], self._world)\n",
    "        \n",
    "        # caveat orginal inclues\n",
    "        \"\"\"\n",
    "        | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        \"\"\"\n",
    "        pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | \\w+(?:'\\w+)*        # words that have ' in between\n",
    "        '''\n",
    "        self._world_words = nltk.regexp_tokenize(self.world(), pattern)\n",
    "        self._world_filtered_words = [word for word in self._world_words if word not in stopwords.words('english')]\n",
    "        \n",
    "\n",
    "        # world term frequency\n",
    "        self.build_world_tf()\n",
    "        \n",
    "    \n",
    "    def world(self):\n",
    "        return self._world\n",
    "    def world_words(self):\n",
    "        return self._world_words\n",
    "    def world_filtered_words(self):\n",
    "        return self._world_filtered_words\n",
    "\n",
    "    def world_tf(self):\n",
    "        return self._world_tf\n",
    "    def world_filtered_tf(self):\n",
    "        return self._world_filtered_tf\n",
    "    \n",
    "    def build_world_tf(self):\n",
    "        from collections import Counter\n",
    "        self._world_tf = Counter(self.world_words())\n",
    "        self._world_filtered_tf = Counter(self.world_filtered_words())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# test for base\n",
    "if not os.path.exists('dataset'):\n",
    "    print('fak, fix it')\n",
    "else:\n",
    "    import codecs\n",
    "    \n",
    "    DOC_PREFIX = 'dataset/text/documents'\n",
    "    txts = os.listdir(DOC_PREFIX)[:6]\n",
    "    counter = 0\n",
    "    docs = deque()\n",
    "    for t in txts:\n",
    "        with codecs.open(os.path.join(DOC_PREFIX,t),'r','utf-8') as f:\n",
    "            raw = f.read()\n",
    "            doc_id = os.path.splitext(os.path.basename(f.name))[0]\n",
    "#             print(doc_id)\n",
    "            docs.append((doc_id,raw))\n",
    "            counter+=1\n",
    "    \n",
    "    tsbase = TSBase()\n",
    "    tsbase.build_sentence_db(docs)\n",
    "#     print(tsbase.world())\n",
    "#     print(tsbase.words())\n",
    "    print(tsbase.world_filtered_tf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# centrality of sentence := \\sum qualified(words); where qualified(w) = if w in pseudo_document ? 1 : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "nltk.regexp_tokenize(text, pattern)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
